{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-15T15:35:14.728675Z","iopub.execute_input":"2021-10-15T15:35:14.729261Z","iopub.status.idle":"2021-10-15T15:35:14.744716Z","shell.execute_reply.started":"2021-10-15T15:35:14.729202Z","shell.execute_reply":"2021-10-15T15:35:14.743943Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"import re\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom collections import defaultdict\n\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, Flatten, GlobalMaxPool1D, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2021-10-15T15:35:14.746037Z","iopub.execute_input":"2021-10-15T15:35:14.746296Z","iopub.status.idle":"2021-10-15T15:35:14.757142Z","shell.execute_reply.started":"2021-10-15T15:35:14.746263Z","shell.execute_reply":"2021-10-15T15:35:14.756379Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('../input/nlp-getting-started/train.csv')\ndf_test = pd.read_csv('../input/nlp-getting-started/train.csv')\ndf_train.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-10-15T15:35:14.759029Z","iopub.execute_input":"2021-10-15T15:35:14.759780Z","iopub.status.idle":"2021-10-15T15:35:14.817535Z","shell.execute_reply.started":"2021-10-15T15:35:14.759747Z","shell.execute_reply":"2021-10-15T15:35:14.816855Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"#Breif info of the dataset\ndf_train.info()","metadata":{"execution":{"iopub.status.busy":"2021-10-15T15:35:14.819694Z","iopub.execute_input":"2021-10-15T15:35:14.819943Z","iopub.status.idle":"2021-10-15T15:35:14.838882Z","shell.execute_reply.started":"2021-10-15T15:35:14.819912Z","shell.execute_reply":"2021-10-15T15:35:14.837976Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"#Distribution of 'target'\nsns.barplot(df_train['target'].value_counts().index, df_train['target'].value_counts())\ndf_train['target'].value_counts(1)","metadata":{"execution":{"iopub.status.busy":"2021-10-15T15:35:14.840103Z","iopub.execute_input":"2021-10-15T15:35:14.840734Z","iopub.status.idle":"2021-10-15T15:35:15.009149Z","shell.execute_reply.started":"2021-10-15T15:35:14.840692Z","shell.execute_reply":"2021-10-15T15:35:15.008326Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"#common keywords happen in real-diaster tweet and fake-diaster tweet\n\ndf_train['target_mean'] = df_train.groupby('keyword')['target'].transform('mean')\n\n#the eleventh common keyowrds in real_diaster is 'bombing', with target_mean : 0.931034\n#append the top 10 keywords happened in real_diaster tweet\n#same process on keyword_fake\n\nkeyword_real = []\nfor kw in df_train.query('target_mean > 0.93104').keyword:\n    if kw not in keyword_real:\n        keyword_real.append(kw)\n        \nkeyword_fake = []\nfor kw in df_train.query('target_mean < 0.0625').keyword:\n    if kw not in keyword_fake:\n        keyword_fake.append(kw)\n\n#output\n#keyword_real = ['debris','derailment','oil%20spill','outbreak','suicide%20bomber','suicide%20bombing','typhoon','wreckage']\n#keyword_fake = ['aftershock','blazing',blew%20up','body%20bag','body%20bags','electrocute','panicking','ruin','screaming','traumatised']","metadata":{"execution":{"iopub.status.busy":"2021-10-15T15:35:15.010501Z","iopub.execute_input":"2021-10-15T15:35:15.018397Z","iopub.status.idle":"2021-10-15T15:35:15.037338Z","shell.execute_reply.started":"2021-10-15T15:35:15.018336Z","shell.execute_reply":"2021-10-15T15:35:15.036655Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"#number of words in a tweet\ndf_train['word_count'] = df_train['text'].apply(lambda x: len(str(x).split()))\n\n#numbers of punctutaion \ndf_train['punctuation_count'] = df_train['text'].apply(lambda x: len([y for y in str(x) if y in string.punctuation]))\n\n#characters in tweets\ndf_train['characters_count'] = df_train['text'].apply(lambda x: len(str(x)))","metadata":{"execution":{"iopub.status.busy":"2021-10-15T15:35:15.039080Z","iopub.execute_input":"2021-10-15T15:35:15.039509Z","iopub.status.idle":"2021-10-15T15:35:15.128194Z","shell.execute_reply.started":"2021-10-15T15:35:15.039472Z","shell.execute_reply":"2021-10-15T15:35:15.127452Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(figsize=(5,5))\nsns.kdeplot(data=df_train, x='word_count', hue='target', fill=True, common_norm=False, palette=\"crest\",alpha=.5,)","metadata":{"execution":{"iopub.status.busy":"2021-10-15T15:35:15.129344Z","iopub.execute_input":"2021-10-15T15:35:15.129640Z","iopub.status.idle":"2021-10-15T15:35:15.420691Z","shell.execute_reply.started":"2021-10-15T15:35:15.129605Z","shell.execute_reply":"2021-10-15T15:35:15.419931Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"sns.kdeplot(data=df_train, x='punctuation_count', hue='target', fill=True, common_norm=False, palette=\"crest\",alpha=.5,)","metadata":{"execution":{"iopub.status.busy":"2021-10-15T15:35:15.421874Z","iopub.execute_input":"2021-10-15T15:35:15.422534Z","iopub.status.idle":"2021-10-15T15:35:15.698647Z","shell.execute_reply.started":"2021-10-15T15:35:15.422499Z","shell.execute_reply":"2021-10-15T15:35:15.697969Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"sns.kdeplot(data=df_train, x='characters_count', hue='target', fill=True, common_norm=False, palette=\"crest\",alpha=.5,)","metadata":{"execution":{"iopub.status.busy":"2021-10-15T15:35:15.701812Z","iopub.execute_input":"2021-10-15T15:35:15.702266Z","iopub.status.idle":"2021-10-15T15:35:15.978780Z","shell.execute_reply.started":"2021-10-15T15:35:15.702229Z","shell.execute_reply":"2021-10-15T15:35:15.977948Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"#analyze common stopwords \n#using the stopwords library from NLTK\ncorpus_fake=[]\nfor x in df_train[df_train['target']==0]['text'].str.split():\n    for i in x:\n        corpus_fake.append(i)\n        \ncorpus_real=[]\nfor x in df_train[df_train['target']==1]['text'].str.split():\n    for i in x:\n        corpus_real.append(i)\n\nstops = set(stopwords.words('english'))\n\ndic_fake = defaultdict(int)\ndic_real = defaultdict(int)\n\nfor word in corpus_fake:\n    if word in stops:\n        dic_fake[word]+=1\n        \nfor word in corpus_real:\n    if word in stops:\n        dic_real[word]+=1","metadata":{"execution":{"iopub.status.busy":"2021-10-15T15:35:15.980030Z","iopub.execute_input":"2021-10-15T15:35:15.980588Z","iopub.status.idle":"2021-10-15T15:35:16.043359Z","shell.execute_reply.started":"2021-10-15T15:35:15.980540Z","shell.execute_reply":"2021-10-15T15:35:16.042680Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"#The most common stopwords in real and fake diaster tweet\nprint(sorted(dic_fake.items(),key=lambda x:x[1], reverse=True)[:10])\nprint(sorted(dic_real.items(),key=lambda x:x[1], reverse=True)[:10])","metadata":{"execution":{"iopub.status.busy":"2021-10-15T15:35:16.044505Z","iopub.execute_input":"2021-10-15T15:35:16.045283Z","iopub.status.idle":"2021-10-15T15:35:16.051803Z","shell.execute_reply.started":"2021-10-15T15:35:16.045241Z","shell.execute_reply":"2021-10-15T15:35:16.050913Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"#unpacking the dict using operator *\ntemp1 = sorted(dic_fake.items(),key=lambda x:x[1], reverse=True)[:10]\ntemp2 = sorted(dic_real.items(),key=lambda x:x[1], reverse=True)[:10]\n\nx1,y1=zip(*temp1)\nx2,y2=zip(*temp2)\n\nplt.figure(figsize=(9,6))\n \n# creating the bar plot\nplt.bar(x1, y1, color ='blue',\n        width = 0.4)\n \nplt.xlabel(\"Stopwords\")\nplt.ylabel(\"Word counts\")\nplt.title(\"Common stopword in fake-diiaster tweet\")\nplt.show()\n\nplt.figure(figsize=(9,6))\n\nplt.bar(x2, y2, color ='palegreen',\n        width = 0.4)\n\nplt.xlabel(\"Stopwords\")\nplt.ylabel(\"Word counts\")\nplt.title(\"Common stopword in real-diiaster tweet\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-15T15:35:16.053568Z","iopub.execute_input":"2021-10-15T15:35:16.053991Z","iopub.status.idle":"2021-10-15T15:35:16.495402Z","shell.execute_reply.started":"2021-10-15T15:35:16.053957Z","shell.execute_reply":"2021-10-15T15:35:16.494725Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"#analyze common hastags\nfrom collections import Counter\n\nhastags_real = [hastag for hastag in corpus_real if '#' in hastag]\nhastags_fake = [hastag for hastag in corpus_fake if '#' in hastag]\n\ntemp1 = sorted(Counter(hastags_fake).items(),key=lambda x:x[1], reverse=True)[:10]\ntemp2 = sorted(Counter(hastags_real).items(),key=lambda x:x[1], reverse=True)[:10]\n\nx1,y1=zip(*temp1)\nx2,y2=zip(*temp2)\n\nplt.figure(figsize=(9,6))\n \n# creating the bar plot\nplt.bar(x1, y1, color ='blue',\n        width = 0.4)\n \nplt.xlabel(\"Hastags\")\nplt.ylabel(\"Hastags counts\")\nplt.title(\"Common hastags in fake-diiaster tweet\")\nplt.show()\n\nplt.figure(figsize=(9,6))\n\nplt.bar(x2, y2, color ='orange',\n        width = 0.4)\n\nplt.xlabel(\"Hastags\")\nplt.ylabel(\"Hastags counts\")\nplt.title(\"Common hastags in real-diaster tweet\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-15T15:35:16.496684Z","iopub.execute_input":"2021-10-15T15:35:16.496955Z","iopub.status.idle":"2021-10-15T15:35:16.953175Z","shell.execute_reply.started":"2021-10-15T15:35:16.496923Z","shell.execute_reply":"2021-10-15T15:35:16.952353Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"#some of the hastages were not able capture when preparaing the dataset","metadata":{"execution":{"iopub.status.busy":"2021-10-15T15:35:16.954540Z","iopub.execute_input":"2021-10-15T15:35:16.955021Z","iopub.status.idle":"2021-10-15T15:35:16.959116Z","shell.execute_reply.started":"2021-10-15T15:35:16.954985Z","shell.execute_reply":"2021-10-15T15:35:16.958516Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning","metadata":{}},{"cell_type":"code","source":"#Make text lowercase, remove text in square brackets,remove URLs,remove punctuation and remove words containing numbers, remove stopwords.\n#Perform stemming using SnowballStemmer","metadata":{"execution":{"iopub.status.busy":"2021-10-15T15:35:16.960327Z","iopub.execute_input":"2021-10-15T15:35:16.961114Z","iopub.status.idle":"2021-10-15T15:35:16.975184Z","shell.execute_reply.started":"2021-10-15T15:35:16.961080Z","shell.execute_reply":"2021-10-15T15:35:16.974339Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"# Special thanks to https://www.kaggle.com/tanulsingh077 for this function\ndef clean_text(text):\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-10-15T15:35:16.978807Z","iopub.execute_input":"2021-10-15T15:35:16.979015Z","iopub.status.idle":"2021-10-15T15:35:16.987411Z","shell.execute_reply.started":"2021-10-15T15:35:16.978987Z","shell.execute_reply":"2021-10-15T15:35:16.986086Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"#remove stopwords using the list of stopwords defined eariler in this notebook\n\ndef remove_stopwords(text):\n    text = ' '.join(word for word in text.split(' ') if word not in stops)\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-10-15T15:35:16.989115Z","iopub.execute_input":"2021-10-15T15:35:16.990398Z","iopub.status.idle":"2021-10-15T15:35:17.000654Z","shell.execute_reply.started":"2021-10-15T15:35:16.990355Z","shell.execute_reply":"2021-10-15T15:35:16.999464Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"#stemmer = nltk.SnowballStemmer(\"english\")\n#\n#def stemm_text(text):\n#    text = ' '.join(stemmer.stem(word) for word in text.split(' '))\n#    return text","metadata":{"execution":{"iopub.status.busy":"2021-10-15T15:35:17.001634Z","iopub.execute_input":"2021-10-15T15:35:17.001838Z","iopub.status.idle":"2021-10-15T15:35:17.009543Z","shell.execute_reply.started":"2021-10-15T15:35:17.001816Z","shell.execute_reply":"2021-10-15T15:35:17.008650Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"#Putting the functions together\ndef preprocess_data(text):\n    text = clean_text(text)\n    text = ' '.join(word for word in text.split(' ') if word not in stops)\n    #text = ' '.join(stemmer.stem(word) for word in text.split(' '))\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-10-15T15:44:09.093603Z","iopub.execute_input":"2021-10-15T15:44:09.094563Z","iopub.status.idle":"2021-10-15T15:44:09.100115Z","shell.execute_reply.started":"2021-10-15T15:44:09.094511Z","shell.execute_reply":"2021-10-15T15:44:09.099338Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"df_train['text_clean'] = df_train['text'].apply(preprocess_data)","metadata":{"execution":{"iopub.status.busy":"2021-10-15T15:44:11.790938Z","iopub.execute_input":"2021-10-15T15:44:11.791865Z","iopub.status.idle":"2021-10-15T15:44:12.075108Z","shell.execute_reply.started":"2021-10-15T15:44:11.791826Z","shell.execute_reply":"2021-10-15T15:44:12.074406Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"df_train.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-10-15T15:44:15.073998Z","iopub.execute_input":"2021-10-15T15:44:15.074790Z","iopub.status.idle":"2021-10-15T15:44:15.091352Z","shell.execute_reply.started":"2021-10-15T15:44:15.074751Z","shell.execute_reply":"2021-10-15T15:44:15.090540Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"markdown","source":"# Vectorization","metadata":{}},{"cell_type":"code","source":"texts = df_train['text_clean']\ntargets = df_train['target']\n\ntokenizer = Tokenizer(oov_token='<OOV>')\ntokenizer.fit_on_texts(texts)\n\nvocab_length = len(tokenizer.word_index) + 1\nvocab_length","metadata":{"execution":{"iopub.status.busy":"2021-10-15T15:44:20.644553Z","iopub.execute_input":"2021-10-15T15:44:20.644811Z","iopub.status.idle":"2021-10-15T15:44:20.785032Z","shell.execute_reply.started":"2021-10-15T15:44:20.644784Z","shell.execute_reply":"2021-10-15T15:44:20.784018Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"sequences = tokenizer.texts_to_sequences(texts)\npadded = pad_sequences(sequences, padding='post')\npadded","metadata":{"execution":{"iopub.status.busy":"2021-10-15T15:44:23.099810Z","iopub.execute_input":"2021-10-15T15:44:23.100548Z","iopub.status.idle":"2021-10-15T15:44:23.254113Z","shell.execute_reply.started":"2021-10-15T15:44:23.100513Z","shell.execute_reply":"2021-10-15T15:44:23.253260Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"#put the word and its corresponding vectors into a dict\nembeddings_dictionary = dict()\nembedding_dim = 100\n\n# Load GloVe 100D embeddings\nwith open('/kaggle/input/glove6b100dtxt/glove.6B.100d.txt') as fp:\n    for line in fp.readlines():\n        records = line.split()\n        word = records[0]\n        vector_dimensions = np.asarray(records[1:], dtype='float32')\n        embeddings_dictionary [word] = vector_dimensions\n\n# embeddings_dictionary","metadata":{"execution":{"iopub.status.busy":"2021-10-15T15:44:25.570693Z","iopub.execute_input":"2021-10-15T15:44:25.570963Z","iopub.status.idle":"2021-10-15T15:44:41.753702Z","shell.execute_reply.started":"2021-10-15T15:44:25.570935Z","shell.execute_reply":"2021-10-15T15:44:41.752664Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"embedding_matrix = np.zeros((vocab_length, embedding_dim))\n\nfor word, index in tokenizer.word_index.items():\n    embedding_vector = embeddings_dictionary.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector\n        \n#embedding_matrix","metadata":{"execution":{"iopub.status.busy":"2021-10-15T15:44:46.212577Z","iopub.execute_input":"2021-10-15T15:44:46.213060Z","iopub.status.idle":"2021-10-15T15:44:46.265851Z","shell.execute_reply.started":"2021-10-15T15:44:46.213016Z","shell.execute_reply":"2021-10-15T15:44:46.265208Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"markdown","source":"# Baseline Model","metadata":{}},{"cell_type":"code","source":"#Building a baseline model and compare the performance of futhure models with this\n\nword_counts = []\nfor sentence in texts:\n    word_counts.append(len(sentence))\n\nlength_long_sentence = max(word_counts)\n\nmodel = Sequential()\nmodel.add(Embedding(input_dim=embedding_matrix.shape[0], output_dim=embedding_matrix.shape[1], weights = [embedding_matrix], input_length=128))  \nmodel.add(Bidirectional(LSTM(128, recurrent_dropout=0.2)))\nmodel.add(Dense(1, activation = 'sigmoid'))\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2021-10-15T15:44:49.448077Z","iopub.execute_input":"2021-10-15T15:44:49.448543Z","iopub.status.idle":"2021-10-15T15:44:49.778522Z","shell.execute_reply.started":"2021-10-15T15:44:49.448509Z","shell.execute_reply":"2021-10-15T15:44:49.777764Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(padded, targets, test_size=0.25, random_state = 42)\nhistory = model.fit(X_train, y_train, epochs = 5,batch_size = 32, validation_data = (X_test, y_test),verbose = 1)","metadata":{"execution":{"iopub.status.busy":"2021-10-15T15:44:56.117920Z","iopub.execute_input":"2021-10-15T15:44:56.118183Z","iopub.status.idle":"2021-10-15T15:47:39.476158Z","shell.execute_reply.started":"2021-10-15T15:44:56.118155Z","shell.execute_reply":"2021-10-15T15:47:39.475469Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"def plot_learning_curves(history, arr):\n    fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n    for idx in range(2):\n        ax[idx].plot(history.history[arr[idx][0]])\n        ax[idx].plot(history.history[arr[idx][1]])\n        ax[idx].legend([arr[idx][0], arr[idx][1]],fontsize=18)\n        ax[idx].set_xlabel('epochs ',fontsize=16)\n        ax[idx].set_title(arr[idx][0] + ' and ' + arr[idx][1],fontsize=16)\n\nplot_learning_curves(history, [['loss', 'val_loss'],['accuracy', 'val_accuracy']])\n\n#the baseline model shows after trained with 5 epochs, the accuracy is 95%, but overfitting may happens after \n#traine with 2 epochs, the val_accuracy and val_loss became unstable after 2 epochs","metadata":{"execution":{"iopub.status.busy":"2021-10-15T15:47:39.480764Z","iopub.execute_input":"2021-10-15T15:47:39.481287Z","iopub.status.idle":"2021-10-15T15:47:39.999470Z","shell.execute_reply.started":"2021-10-15T15:47:39.481250Z","shell.execute_reply":"2021-10-15T15:47:39.998743Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"markdown","source":"# Complete model","metadata":{}},{"cell_type":"code","source":"#Callback functions EarlyStopping, and ReduceLROnPlateau added\n#Introducing more layers to imporve accuracy\n#Dropout functions to prevent overfitting problem\n\nearly_stop = EarlyStopping(monitor='val_loss',patience=5,min_delta=0.0000001,restore_best_weights=True)\n\nreduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, verbose = 1, patience = 5, min_lr = 0.001)\n    \nmodel = Sequential()\nmodel.add(Embedding(input_dim=embedding_matrix.shape[0], output_dim=embedding_matrix.shape[1], weights = [embedding_matrix], input_length=length_long_sentence))\nmodel.add(Bidirectional(LSTM(length_long_sentence, return_sequences = True, recurrent_dropout=0.2)))\nmodel.add(GlobalMaxPool1D()) #used for flatterning\nmodel.add(BatchNormalization()) #standardizing the value, and speed up the training process\nmodel.add(Dropout(0.5))\nmodel.add(Dense(length_long_sentence, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(length_long_sentence, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation = 'sigmoid'))\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2021-10-15T15:47:45.010207Z","iopub.execute_input":"2021-10-15T15:47:45.011036Z","iopub.status.idle":"2021-10-15T15:47:45.278603Z","shell.execute_reply.started":"2021-10-15T15:47:45.010989Z","shell.execute_reply":"2021-10-15T15:47:45.277913Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(padded, targets, test_size=0.25, random_state = 42)\nhistory = model.fit(X_train, y_train, epochs = 20, batch_size = 32, validation_data = (X_test, y_test),verbose = 2, callbacks=[early_stop, reduce_lr])","metadata":{"execution":{"iopub.status.busy":"2021-10-15T15:47:47.817722Z","iopub.execute_input":"2021-10-15T15:47:47.818287Z","iopub.status.idle":"2021-10-15T15:53:13.918016Z","shell.execute_reply.started":"2021-10-15T15:47:47.818251Z","shell.execute_reply":"2021-10-15T15:53:13.917358Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"def plot_learning_curves(history, arr):\n    fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n    for idx in range(2):\n        ax[idx].plot(history.history[arr[idx][0]])\n        ax[idx].plot(history.history[arr[idx][1]])\n        ax[idx].legend([arr[idx][0], arr[idx][1]],fontsize=18)\n        ax[idx].set_xlabel('epochs ',fontsize=16)\n        ax[idx].set_title(arr[idx][0] + ' and ' + arr[idx][1],fontsize=16)\n\nplot_learning_curves(history, [['loss', 'val_loss'],['accuracy', 'val_accuracy']])\n#after 4 epochs the val_loss gone up","metadata":{"execution":{"iopub.status.busy":"2021-10-15T15:53:53.775647Z","iopub.execute_input":"2021-10-15T15:53:53.775907Z","iopub.status.idle":"2021-10-15T15:53:54.194801Z","shell.execute_reply.started":"2021-10-15T15:53:53.775879Z","shell.execute_reply":"2021-10-15T15:53:54.194105Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\ny_preds = (model.predict(X_test) > 0.5).astype(\"int32\")\nprint(classification_report(y_test, y_preds))","metadata":{"execution":{"iopub.status.busy":"2021-10-15T15:54:04.471296Z","iopub.execute_input":"2021-10-15T15:54:04.471561Z","iopub.status.idle":"2021-10-15T15:54:05.535830Z","shell.execute_reply.started":"2021-10-15T15:54:04.471532Z","shell.execute_reply":"2021-10-15T15:54:05.535110Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"markdown","source":"# Classical machine leanring models","metadata":{}},{"cell_type":"code","source":"#compare the rnn model with one of the classocal machine learning models - naive_bayes\n#based on the metrics F1, since the submissions are evaluated using F1 between the predicted and expected answers.\n\nfrom sklearn.naive_bayes import MultinomialNB\nX_train, X_test, y_train, y_test = train_test_split(padded, targets, test_size=0.25, random_state = 42)\n\nnb = MultinomialNB()\nnb.fit(X_train, y_train)\n\ny_pred_class = nb.predict(X_test)\ny_pred_prob = nb.predict_proba(X_test)[:, 1]\n\nfrom sklearn import metrics\nprint(metrics.accuracy_score(y_test, y_pred_class))\nprint(metrics.confusion_matrix(y_test, y_pred_class))\nprint(classification_report(y_test, y_pred_class))\n\n#The navie bayes is performing badly compare to the deep learning model, it could due to the GloVe embedding used\n#when preparing the train and test data, CountVectorizer from sklearn library does the job better in classical machine learning models","metadata":{"execution":{"iopub.status.busy":"2021-10-15T15:43:37.671362Z","iopub.execute_input":"2021-10-15T15:43:37.671698Z","iopub.status.idle":"2021-10-15T15:43:37.697183Z","shell.execute_reply.started":"2021-10-15T15:43:37.671660Z","shell.execute_reply":"2021-10-15T15:43:37.696298Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"markdown","source":"# Create Submission","metadata":{}},{"cell_type":"code","source":"submission = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')\n\ntest['text_clean'] = test['text'].apply(preprocess_data)\ntexts = test['text_clean']\n\ntokenizer = Tokenizer(oov_token='<OOV>')\ntokenizer.fit_on_texts(texts)\nvocab_length = len(tokenizer.word_index) + 1\nsequences = tokenizer.texts_to_sequences(texts)\npadded_test = pad_sequences(sequences, padding='post')","metadata":{"execution":{"iopub.status.busy":"2021-10-15T15:54:15.766467Z","iopub.execute_input":"2021-10-15T15:54:15.766730Z","iopub.status.idle":"2021-10-15T15:54:16.031800Z","shell.execute_reply.started":"2021-10-15T15:54:15.766704Z","shell.execute_reply":"2021-10-15T15:54:16.031042Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"y_preds = (model.predict(padded_test) > 0.5).astype(\"int32\")\nsub=pd.DataFrame({'id':submission['id'].values.tolist(),'target':y_preds.reshape(3263)})\nsub.to_csv('submission_update.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-15T15:55:09.612843Z","iopub.execute_input":"2021-10-15T15:55:09.613548Z","iopub.status.idle":"2021-10-15T15:55:10.730338Z","shell.execute_reply.started":"2021-10-15T15:55:09.613508Z","shell.execute_reply":"2021-10-15T15:55:10.729608Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}